
        # Check depth map.
        rays_d_cam = self.rays_d_cam[0].to('cpu').detach().numpy().copy()
        frame_mask = frame_mask.to('cpu').detach().numpy().copy()
        frame_depth_map = frame_depth_map.to('cpu').detach().numpy().copy()
        frame_camera_pos = frame_camera_pos.to('cpu').detach().numpy().copy()
        frame_camera_rot = frame_camera_rot.to('cpu').detach().numpy().copy()

        fig = plt.figure()
        ax = Axes3D(fig)
        ax.set_xlabel("X")
        ax.set_ylabel("Y")
        ax.set_zlabel("Z")

        for i in range(5):
            batch_ind = 0
            frame_ind = i
            point = (frame_depth_map[batch_ind, frame_ind][..., None] * rays_d_cam)[frame_mask[batch_ind, frame_ind]].reshape(-1, 3)
            point = np.sum(point[:, None, :] * frame_camera_rot[batch_ind, frame_ind].T, -1) + frame_camera_pos[batch_ind, frame_ind]
            ax.scatter(point[::3, 0], point[::3, 1], point[::3, 2], marker="o", linestyle='None', c='m', s=0.05)

        ax.view_init(elev=45, azim=45)
        fig.savefig("tes.png")
        plt.close()




        # Check depth map.
        batch_ind = 1
        rays_d_cam = self.rays_d_cam[0].to('cpu').detach().numpy().copy()
        frame_mask = frame_mask.to('cpu').detach().numpy().copy()
        frame_depth_map = frame_depth_map.to('cpu').detach().numpy().copy()
        frame_camera_pos = frame_camera_pos.to('cpu').detach().numpy().copy()
        frame_camera_rot = frame_camera_rot.to('cpu').detach().numpy().copy()
        obj_R_wrd = batch_pi2rot_y(-frame_obj_rot[batch_ind]).to('cpu').detach().numpy().copy()

        fig = plt.figure()
        ax = Axes3D(fig)
        ax.set_xlabel("X")
        ax.set_ylabel("Y")
        ax.set_zlabel("Z")

        for i in range(5):
            frame_ind = i
            obj_R_cam = obj_R_wrd[frame_ind] @ frame_camera_rot[batch_ind, frame_ind]# 物体基準座標系へ投影
            point = (frame_depth_map[batch_ind, frame_ind][..., None] * rays_d_cam)[frame_mask[batch_ind, frame_ind]].reshape(-1, 3)
            point = np.sum(point[:, None, :] * frame_camera_rot[batch_ind, frame_ind].T, -1) + frame_camera_pos[batch_ind, frame_ind]
            point = np.sum(point[:, None, :] * obj_R_wrd[frame_ind].T, -1)
            ax.scatter(point[::3, 0], point[::3, 1], point[::3, 2], marker="o", linestyle='None', c='m', s=0.05)

        ax.view_init(elev=45, azim=45)
        fig.savefig("aaa.png")
        plt.close()

        # Check depth map.
        fig = plt.figure()
        ax = Axes3D(fig)
        ax.set_xlabel("X")
        ax.set_ylabel("Y")
        ax.set_zlabel("Z")

        check_data = pickle_load('dataset/dugon/test_moving_camera/test_moving_camera_train_data/1a6f615e8b1b5ae4dbbc9440457e303e/00001.pickle')
        mask = check_data['mask']
        depth_map = torch.from_numpy(check_data['depth_map'].astype(np.float32)).clone()
        # カメラ座標系での物体の点群
        point = (depth_map[0][..., None] * rays_d_cam)[mask[0]]
        ax.scatter(point[::3, 0], point[::3, 1], point[::3, 2], marker="o", linestyle='None', c='m', s=0.05)

        ##################################################
        ##################################################
        ##################################################
        # 回転行列の各成分が物体座標系のｘｙｚ各軸の向き
        # 回転行列のｘ，ｙ成分を確認
        obj_rot = torch.from_numpy(check_data['obj_rot'].astype(np.float32)).clone()
        obj_R_wrd = batch_pi2rot_y(-obj_rot).to('cpu').detach().numpy().copy()[0]
        obj_R_cam = check_data['camera_rot'][0]@obj_R_wrd
        axis_green = obj_R_cam[:, 1] # Y
        axis_red = obj_R_cam[:, 0] # minus_X
        axis_blue = np.cross(axis_red, axis_green) # Z
        x, y, z = [], [], [] # 0.0, 0.0, 0.5
        u, v, w = [], [], [] # 0.5, 0.0, 0.0
        for axis in [axis_green, axis_red, axis_blue]:
            x.append(0.0)
            y.append(0.0)
            z.append(0.5)
            u.append(0.3*axis[0].item())
            v.append(0.3*axis[1].item())
            w.append(0.3*axis[2].item())
        ax.quiver(x, y, z, u, v, w, arrow_length_ratio=0.1)
        # ##################################################
        # ##################################################
        # ##################################################
        # # obj_R_camによって点群を物体座標系へ戻す
        # obj_R_cam[:, 0] = -obj_R_cam[:, 0]
        # point = torch.sum(point[:, None, :] * obj_R_cam.T[None], -1)
        # ax.scatter(point[::3, 0], point[::3, 1], point[::3, 2], marker="o", linestyle='None', c='c', s=0.05)
        # ##################################################
        # ##################################################
        # ##################################################

        # ax.set_xlim(-.5, .5) # x軸の表示範囲
        # ax.set_ylim(-.5, .5) # y軸の表示範囲
        # ax.set_zlim(0, 1) # z軸の表示範囲

        # ax.view_init(elev=0, azim=90)
        ax.view_init(elev=45, azim=45)
        fig.savefig("tes.png")
        # plt.close()





        # Get depth map.
        # w2o = batch_pi2rot_y(-frame_obj_rot[:, 0])[0].T
        # test_pos = (w2o@frame_camera_pos[0, 0]).unsqueeze(0)
        # c2w = frame_camera_rot[0, 0].T
        # test_c2w = (w2o@c2w).unsqueeze(0) # camera -> world
        test_pos = pos_obj[0].unsqueeze(0)
        test_c2w = pos_c2w[0].unsqueeze(0) # camera -> world
        test_instance_idx = self.ddf_instance_list.index(instance_id[0])
        test_idx = torch.full((1, 1), test_instance_idx).to(self.ddf.device)
        # input for self.ddf.forward()
        input_lat_vec = self.ddf.lat_vecs(test_idx)
        rays_o = test_pos[:, None, None, :].expand(-1, self.H, self.H, -1)
        rays_d_wrd = torch.sum(self.rays_d_cam[:, :, :, None, :].to(self.ddf.device) * test_c2w[:, None, None, :, :], -1)
        # rendering
        est_invdepth_map = self.ddf.forward(rays_o, rays_d_wrd, input_lat_vec)
        check_map(est_invdepth_map[0])
        check_map(frame_depth_map[0, 0], 'tes_ori.png')





        loss = []
        frame_num = frame_rgb_map.shape[1]
        for frame_idx in range(frame_num):

            depth_map = frame_depth_map[:, frame_idx]
            mask = frame_mask[:, frame_idx]

            inp = torch.stack([depth_map, mask], 1)
            loss_i = torch.norm(self.model(inp))
            loss.append(loss_i)